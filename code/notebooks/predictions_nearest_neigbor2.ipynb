{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook, I would like to predicting power fails in 10 days periods.**\n",
    "===========================================\n",
    "1. Generate a dataset for this task\n",
    "2. Try out and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import re\n",
    "from IPython.display import Image\n",
    "import itertools\n",
    "import folium\n",
    "import pandas as pd\n",
    "import folium\n",
    "from IPython.display import display\n",
    "import geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up Working Directory and Data Files\n",
    "drive = 'G'\n",
    "if drive == 'G':\n",
    "    data_dir = '../01.data/'\n",
    "    output_dir = '../05.outputs/'\n",
    "else:\n",
    "    data_dir = '..\\\\01.data\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sms_v2=\"/Users/dmatekenya/Google Drive/World-Bank/electricity_monitoring/01.data/sms_v2_2017-08-06.csv\"\n",
    "cols_to_use = ['box_id', 'psu', 'lon', 'lat', 'datetime_sent_raw', 'str_datetime_sent',\n",
    "                   'str_datetime_sent_hr', 'hour_sent', 'event_type_str','event_type_num',\n",
    "                   'power_state','data_source']\n",
    "\n",
    "df = pd.read_csv(file_sms_v2, usecols=cols_to_use, parse_dates=['str_datetime_sent_hr'], nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pon_mon      5237\n",
       "missing      1827\n",
       "pfail        1303\n",
       "pback        1158\n",
       "pfail_mon     449\n",
       "test           26\n",
       "Name: event_type_str, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.event_type_str.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  poweroutage_counts (df_power_outage,df_box,time_intervals):\n",
    "    \"\"\"\n",
    "    This function computes number of outages per unit of time\n",
    "    for all the events which happened when the boxes were functioning.\n",
    "    :param power_outage_source: csv file of power outages processed by JP\n",
    "    :param time_intervals: e.g. daily\n",
    "    :returns a pd.DataFrame object with the following columns: BoxID, lat/lon, time_unit, count\n",
    "    \"\"\"\n",
    "\n",
    "    if time_intervals == 'daily':\n",
    "        df = df_power_outage.groupby(['BoxID','date_powerfailure'])['POWERout'].count()\n",
    "        by_box = df.reset_index()\n",
    "    \n",
    "        #merge the two datasets\n",
    "        by_box2 = pd.merge (left = by_box, right=bx, on='BoxID', how='left')\n",
    "        #powerout count by location\n",
    "        by_box3 = by_box2.groupby(['lon','lat','date_powerfailure'])['POWERout'].count()\n",
    "        by_latlon = by_box3.reset_index()\n",
    "        \n",
    "        #prettify the DF\n",
    "        by_latlon.rename(columns={'date_powerfailure': 'date', 'POWERout': 'outage_freq'}, inplace=True)\n",
    "        \n",
    "        return by_latlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_powerout_data(file_name):\n",
    "    \"\"\"\n",
    "    Does some data wrangling for use in the prediction of average duration\n",
    "    :param file_name: csv file of power outages processed by JP\n",
    "    :returns a pd.DataFrame object\n",
    "    \"\"\"\n",
    "    # Load data of hourly power events: data prpcessed by JP\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    # Kepp only required columns\n",
    "    to_keep = ['BoxID', 'date_powerfailure', 'date_powerfailure_hour',\n",
    "               'date_powerback', 'date_powerback_hour', 'duration', 'POWERout']\n",
    "\n",
    "    d = df[to_keep]\n",
    "    d.is_copy = False\n",
    "\n",
    "    # Change duration to hours\n",
    "    d['duration_hrs'] = d['duration'] / 60\n",
    "\n",
    "    # consider only valid power out events\n",
    "    d = d[d['duration_hrs'].notnull()]\n",
    "\n",
    "    # change to datetime objects\n",
    "    d['date_powerfailure2'] = d.apply(lambda x: pd.datetime.strptime(x['date_powerfailure'], '%d%b%Y'), axis=1)\n",
    "\n",
    "    d.drop('date_powerfailure', axis=1, inplace=True)\n",
    "\n",
    "    d.rename(columns={'date_powerfailure2': 'date_powerfailure'}, inplace=True)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = preprocess_powerout_data(data_dir + 'powerout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_d = d.groupby(['BoxID','date_powerfailure'])['POWERout'].count()\n",
    "by_box = d_d.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'date': pd.date_range (d.date_powerfailure.min(),d.date_powerfailure.max())})\n",
    "df['outage_freq'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_latlon_full = pd.merge (left = df, right=by_latlon, left_on='date', right_on='date_powerfailure', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 2)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_latlon['latlon'] = by_latlon.apply (lambda row: str(row['lon']) + ','+ str(row['lat']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>date_powerfailure</th>\n",
       "      <th>POWERout</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.533566</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>2</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.533566</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>2016-12-03</td>\n",
       "      <td>2</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.533566</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>2</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.533566</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.533566</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>2016-12-07</td>\n",
       "      <td>2</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lon        lat date_powerfailure  POWERout                      loc\n",
       "0  38.533566  68.862146        2016-12-02         2  38.53356644,68.86214562\n",
       "1  38.533566  68.862146        2016-12-03         2  38.53356644,68.86214562\n",
       "2  38.533566  68.862146        2016-12-04         2  38.53356644,68.86214562\n",
       "3  38.533566  68.862146        2016-12-06         2  38.53356644,68.86214562\n",
       "4  38.533566  68.862146        2016-12-07         2  38.53356644,68.86214562"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import box details\n",
    "bx = pd.read_csv(data_dir + 'Boxes.csv', usecols=['LONG','LAT','ClusterId','BoxID'])\n",
    "#rename some cols\n",
    "bx.rename (columns={'LONG': 'lon','LAT':'lat'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge the two datasets\n",
    "by_box2 = pd.merge (left = by_box, right=bx, on='BoxID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq_locs = list (by_latlon.latlon.unique())\n",
    "df_full = pd.DataFrame()\n",
    "for l in uniq_locs:\n",
    "    df_loc = by_latlon[by_latlon.latlon==l]\n",
    "    merged= pd.merge (left = df, right=df_loc,left_on='date', right_on='date_powerfailure', how='left')\n",
    "    merged ['outage_freq'] = merged.apply (lambda x: max (x['outage_freq'],x['POWERout']),axis=1)\n",
    "    merged['latlon'] = l\n",
    "    merged['lon'] = float(l.split(',')[1])\n",
    "    merged['lat'] = float(l.split(',')[0])\n",
    "    df_full = df_full.append(merged)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>outage_freq</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>date_powerfailure</th>\n",
       "      <th>POWERout</th>\n",
       "      <th>loc</th>\n",
       "      <th>latlon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-10-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-10-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-10-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-10-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-10-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-11-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-11-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-11-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-11-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-11-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-11-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-11-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-11-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-11-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-11-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-11-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-11-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-11-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016-11-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2016-11-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016-11-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016-11-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.862146</td>\n",
       "      <td>38.533566</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.53356644,68.86214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2017-02-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2017-02-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>2017-02-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2017-02-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2017-02-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2017-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2017-03-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2017-03-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2017-03-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2017-03-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2017-03-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2017-03-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2017-03-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2017-03-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>2017-03-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2017-03-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2017-03-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2017-03-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2017-03-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2017-03-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.699196</td>\n",
       "      <td>72.336579</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.3365791,37.6991956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21267 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  outage_freq        lon        lat date_powerfailure  POWERout  \\\n",
       "0   2016-10-22          0.0  68.862146  38.533566               NaT       NaN   \n",
       "1   2016-10-23          0.0  68.862146  38.533566               NaT       NaN   \n",
       "2   2016-10-24          0.0  68.862146  38.533566               NaT       NaN   \n",
       "3   2016-10-25          0.0  68.862146  38.533566               NaT       NaN   \n",
       "4   2016-10-26          0.0  68.862146  38.533566               NaT       NaN   \n",
       "5   2016-10-27          0.0  68.862146  38.533566               NaT       NaN   \n",
       "6   2016-10-28          0.0  68.862146  38.533566               NaT       NaN   \n",
       "7   2016-10-29          0.0  68.862146  38.533566               NaT       NaN   \n",
       "8   2016-10-30          0.0  68.862146  38.533566               NaT       NaN   \n",
       "9   2016-10-31          0.0  68.862146  38.533566               NaT       NaN   \n",
       "10  2016-11-01          0.0  68.862146  38.533566               NaT       NaN   \n",
       "11  2016-11-02          0.0  68.862146  38.533566               NaT       NaN   \n",
       "12  2016-11-03          0.0  68.862146  38.533566               NaT       NaN   \n",
       "13  2016-11-04          0.0  68.862146  38.533566               NaT       NaN   \n",
       "14  2016-11-05          0.0  68.862146  38.533566               NaT       NaN   \n",
       "15  2016-11-06          0.0  68.862146  38.533566               NaT       NaN   \n",
       "16  2016-11-07          0.0  68.862146  38.533566               NaT       NaN   \n",
       "17  2016-11-08          0.0  68.862146  38.533566               NaT       NaN   \n",
       "18  2016-11-09          0.0  68.862146  38.533566               NaT       NaN   \n",
       "19  2016-11-10          0.0  68.862146  38.533566               NaT       NaN   \n",
       "20  2016-11-11          0.0  68.862146  38.533566               NaT       NaN   \n",
       "21  2016-11-12          0.0  68.862146  38.533566               NaT       NaN   \n",
       "22  2016-11-13          0.0  68.862146  38.533566               NaT       NaN   \n",
       "23  2016-11-14          0.0  68.862146  38.533566               NaT       NaN   \n",
       "24  2016-11-15          0.0  68.862146  38.533566               NaT       NaN   \n",
       "25  2016-11-16          0.0  68.862146  38.533566               NaT       NaN   \n",
       "26  2016-11-17          0.0  68.862146  38.533566               NaT       NaN   \n",
       "27  2016-11-18          0.0  68.862146  38.533566               NaT       NaN   \n",
       "28  2016-11-19          0.0  68.862146  38.533566               NaT       NaN   \n",
       "29  2016-11-20          0.0  68.862146  38.533566               NaT       NaN   \n",
       "..         ...          ...        ...        ...               ...       ...   \n",
       "123 2017-02-22          0.0  37.699196  72.336579               NaT       NaN   \n",
       "124 2017-02-23          1.0  37.699196  72.336579        2017-02-23       1.0   \n",
       "125 2017-02-24          0.0  37.699196  72.336579               NaT       NaN   \n",
       "126 2017-02-25          0.0  37.699196  72.336579               NaT       NaN   \n",
       "127 2017-02-26          0.0  37.699196  72.336579               NaT       NaN   \n",
       "128 2017-02-27          0.0  37.699196  72.336579               NaT       NaN   \n",
       "129 2017-02-28          0.0  37.699196  72.336579               NaT       NaN   \n",
       "130 2017-03-01          0.0  37.699196  72.336579               NaT       NaN   \n",
       "131 2017-03-02          0.0  37.699196  72.336579               NaT       NaN   \n",
       "132 2017-03-03          0.0  37.699196  72.336579               NaT       NaN   \n",
       "133 2017-03-04          0.0  37.699196  72.336579               NaT       NaN   \n",
       "134 2017-03-05          0.0  37.699196  72.336579               NaT       NaN   \n",
       "135 2017-03-06          0.0  37.699196  72.336579               NaT       NaN   \n",
       "136 2017-03-07          0.0  37.699196  72.336579               NaT       NaN   \n",
       "137 2017-03-08          0.0  37.699196  72.336579               NaT       NaN   \n",
       "138 2017-03-09          0.0  37.699196  72.336579               NaT       NaN   \n",
       "139 2017-03-10          0.0  37.699196  72.336579               NaT       NaN   \n",
       "140 2017-03-11          0.0  37.699196  72.336579               NaT       NaN   \n",
       "141 2017-03-12          0.0  37.699196  72.336579               NaT       NaN   \n",
       "142 2017-03-13          0.0  37.699196  72.336579               NaT       NaN   \n",
       "143 2017-03-14          1.0  37.699196  72.336579        2017-03-14       1.0   \n",
       "144 2017-03-15          0.0  37.699196  72.336579               NaT       NaN   \n",
       "145 2017-03-16          0.0  37.699196  72.336579               NaT       NaN   \n",
       "146 2017-03-17          0.0  37.699196  72.336579               NaT       NaN   \n",
       "147 2017-03-18          0.0  37.699196  72.336579               NaT       NaN   \n",
       "148 2017-03-19          0.0  37.699196  72.336579               NaT       NaN   \n",
       "149 2017-03-20          0.0  37.699196  72.336579               NaT       NaN   \n",
       "150 2017-03-21          0.0  37.699196  72.336579               NaT       NaN   \n",
       "151 2017-03-22          0.0  37.699196  72.336579               NaT       NaN   \n",
       "152 2017-03-23          0.0  37.699196  72.336579               NaT       NaN   \n",
       "\n",
       "                       loc                   latlon  \n",
       "0                      NaN  38.53356644,68.86214562  \n",
       "1                      NaN  38.53356644,68.86214562  \n",
       "2                      NaN  38.53356644,68.86214562  \n",
       "3                      NaN  38.53356644,68.86214562  \n",
       "4                      NaN  38.53356644,68.86214562  \n",
       "5                      NaN  38.53356644,68.86214562  \n",
       "6                      NaN  38.53356644,68.86214562  \n",
       "7                      NaN  38.53356644,68.86214562  \n",
       "8                      NaN  38.53356644,68.86214562  \n",
       "9                      NaN  38.53356644,68.86214562  \n",
       "10                     NaN  38.53356644,68.86214562  \n",
       "11                     NaN  38.53356644,68.86214562  \n",
       "12                     NaN  38.53356644,68.86214562  \n",
       "13                     NaN  38.53356644,68.86214562  \n",
       "14                     NaN  38.53356644,68.86214562  \n",
       "15                     NaN  38.53356644,68.86214562  \n",
       "16                     NaN  38.53356644,68.86214562  \n",
       "17                     NaN  38.53356644,68.86214562  \n",
       "18                     NaN  38.53356644,68.86214562  \n",
       "19                     NaN  38.53356644,68.86214562  \n",
       "20                     NaN  38.53356644,68.86214562  \n",
       "21                     NaN  38.53356644,68.86214562  \n",
       "22                     NaN  38.53356644,68.86214562  \n",
       "23                     NaN  38.53356644,68.86214562  \n",
       "24                     NaN  38.53356644,68.86214562  \n",
       "25                     NaN  38.53356644,68.86214562  \n",
       "26                     NaN  38.53356644,68.86214562  \n",
       "27                     NaN  38.53356644,68.86214562  \n",
       "28                     NaN  38.53356644,68.86214562  \n",
       "29                     NaN  38.53356644,68.86214562  \n",
       "..                     ...                      ...  \n",
       "123                    NaN    72.3365791,37.6991956  \n",
       "124  72.3365791,37.6991956    72.3365791,37.6991956  \n",
       "125                    NaN    72.3365791,37.6991956  \n",
       "126                    NaN    72.3365791,37.6991956  \n",
       "127                    NaN    72.3365791,37.6991956  \n",
       "128                    NaN    72.3365791,37.6991956  \n",
       "129                    NaN    72.3365791,37.6991956  \n",
       "130                    NaN    72.3365791,37.6991956  \n",
       "131                    NaN    72.3365791,37.6991956  \n",
       "132                    NaN    72.3365791,37.6991956  \n",
       "133                    NaN    72.3365791,37.6991956  \n",
       "134                    NaN    72.3365791,37.6991956  \n",
       "135                    NaN    72.3365791,37.6991956  \n",
       "136                    NaN    72.3365791,37.6991956  \n",
       "137                    NaN    72.3365791,37.6991956  \n",
       "138                    NaN    72.3365791,37.6991956  \n",
       "139                    NaN    72.3365791,37.6991956  \n",
       "140                    NaN    72.3365791,37.6991956  \n",
       "141                    NaN    72.3365791,37.6991956  \n",
       "142                    NaN    72.3365791,37.6991956  \n",
       "143  72.3365791,37.6991956    72.3365791,37.6991956  \n",
       "144                    NaN    72.3365791,37.6991956  \n",
       "145                    NaN    72.3365791,37.6991956  \n",
       "146                    NaN    72.3365791,37.6991956  \n",
       "147                    NaN    72.3365791,37.6991956  \n",
       "148                    NaN    72.3365791,37.6991956  \n",
       "149                    NaN    72.3365791,37.6991956  \n",
       "150                    NaN    72.3365791,37.6991956  \n",
       "151                    NaN    72.3365791,37.6991956  \n",
       "152                    NaN    72.3365791,37.6991956  \n",
       "\n",
       "[21267 rows x 8 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_loc = by_latlon[by_latlon.latlon==uniq_locs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_full = pd.DataFrame()\n",
    "merged =pd.merge (left = df, right=df_loc,left_on='date', right_on='date_powerfailure', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged ['outage_freq'] = merged.apply (lambda x: max (x['outage_freq'],x['POWERout']),axis=1)\n",
    "merged['latlon'] = uniq_locs[0]\n",
    "merged['lon'] = float(l.split(',')[1])\n",
    "merged['lat'] = float(l.split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.53356644"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(uniq_locs[0].split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_latlon.POWERout.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'38.53356644,68.86214562'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_locs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = preprocess_powerout_data(data_dir + 'powerout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_d = d.groupby(['BoxID','date_powerfailure'])['POWERout'].count()\n",
    "\n",
    "by_box = d_d.reset_index()\n",
    "\n",
    "#add box locaiton details\n",
    "#import box details\n",
    "bx = pd.read_csv(data_dir + 'Boxes.csv', usecols=['LONG','LAT','ClusterId','BoxID'])\n",
    "#rename some cols\n",
    "bx.rename (columns={'LONG': 'lon','LAT':'lat'},inplace=True)\n",
    "    \n",
    "#merge the two datasets\n",
    "by_box2 = pd.merge (left = by_box, right=bx, on='BoxID', how='left')\n",
    "\n",
    "#powerout count by location\n",
    "df = by_box2.groupby(['lon','lat','date_powerfailure'])['POWERout'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = df.ix[(df['lon']==38.533566) & (df['lat'] == 68.862146)]\n",
    "foo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_duration (df, cut_off_date):\n",
    "    '''\n",
    "    The idea is to track number of hours when power is off not\n",
    "    later than some given cut off date by box.\n",
    "    @param:  df: the data file under consideration\n",
    "    @param:  cut_off_date: upper date limit for considerin events\n",
    "    @return: a pd.DataFrame object containing average duration of each box\n",
    "    '''\n",
    "    #select events earlier than cut off date\n",
    "    d_date = df[df.date_powerfailure <= cut_off_date]\n",
    "    \n",
    "    #select only powerout events\n",
    "    d_out = d_date[d_date.duration_hrs.notnull()]\n",
    "    \n",
    "    #Exclude all outages longer than 1 day\n",
    "    print ('Leaving out %s events whose power out duration is over 24 hrs!!!' %(len(d_out[d_out.duration_hrs>=24])))\n",
    "    \n",
    "    d_out = d_out[d_out.duration_hrs < 24]\n",
    "    \n",
    "    #Group by box id and compute mean\n",
    "    by_box = d_out.groupby(['BoxID'])['duration_hrs'].mean()\n",
    "    by_box = by_box.reset_index()\n",
    "    \n",
    "    #add box locaiton details\n",
    "    #import box details\n",
    "    bx = pd.read_csv(data_dir + 'Boxes.csv', usecols=['LONG','LAT','ClusterId','BoxID'])\n",
    "    #rename some cols\n",
    "    bx.rename (columns={'LONG': 'lon','LAT':'lat'},inplace=True)\n",
    "    \n",
    "    #merge the two datasets\n",
    "    by_box2 = pd.merge (left = by_box, right=bx, on='BoxID', how='left')\n",
    "\n",
    "    #merge again by psu-lat/lon\n",
    "    by_xy = by_box2.groupby(['ClusterId','lon','lat'])['duration_hrs'].mean()\n",
    "    \n",
    "    print ('Completed adding mean to df.....')\n",
    "    \n",
    "    return by_xy.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_xy = compute_mean_duration (d,date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = d.date_powerfailure.max()\n",
    "\n",
    "d_date = d[d <= date]\n",
    "\n",
    "d_out = d[d.duration_hrs.notnull()]\n",
    "    \n",
    "d_out = d_out[d_out.duration_hrs < 24]\n",
    "\n",
    "by_box = d_out.groupby(['BoxID'])['duration_hrs'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_box = by_box.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_box2 = pd.merge (left = by_box, right=bx, on='BoxID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_box2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_box2 [by_box2.ClusterId==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_loc=by_box2.groupby(['ClusterId','lon','lat'])['duration_hrs'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array([0.977412,0.481998]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2geojson(df):\n",
    "    features = []\n",
    "    df.apply(lambda X: features.append( \n",
    "            geojson.Feature(geometry=geojson.Point((X[\"lat\"], \n",
    "                                                    X[\"lon\"])), \n",
    "                properties=dict(name=X[\"ClusterId\"], \n",
    "                                description=X[\"avg_duration\"]))\n",
    "                                    )\n",
    "            , axis=1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat = data2geojson(by_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_out = by_loc.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_out.to_csv (data_dir + 'powerout_avg_duration_by_loc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kepp only required columns\n",
    "to_keep = ['BoxID', 'date_powerfailure', 'date_powerfailure_hour',\n",
    "       'date_powerback', 'date_powerback_hour','duration','POWERout']\n",
    "\n",
    "d = d[to_keep]\n",
    "\n",
    "#Change duration to hours\n",
    "d['duration_hrs'] = d.apply(lambda x: x['duration']/60,axis=1)\n",
    "\n",
    "#change to datetime objects\n",
    "d['date_powerfailure'] = d.apply(lambda x: pd.datetime.strptime(x['date_powerfailure'], '%d%b%Y'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str = ''\n",
    "\n",
    "if str is None:\n",
    "    print ('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_out.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_duration (df, cut_off_date):\n",
    "    '''\n",
    "    The idea is to track number of hours when power is off not\n",
    "    later than some given cut off date by box.\n",
    "    @param:  df: the data file under consideration\n",
    "    @param:  cut_off_date: upper date limit for considerin events\n",
    "    @return: a pd.DataFrame object with two columns containing average duration of each box\n",
    "    '''\n",
    "    #select events earlier than cut off date\n",
    "    d_date = d[d <= date]\n",
    "    \n",
    "    #selec only powerout events\n",
    "    d_out = d[d.duration_hrs.notnull()]\n",
    "    \n",
    "    #Exclude all outages longer than 2 days ()\n",
    "    print ('Leaving out %s events whose power out duration is over 24 hrs!!!' %(len(d_out[d_out.duration_hrs>=24])))\n",
    "    d_out = d_out[d_out.duration_hrs < 24]\n",
    "    #Group by box id and compute mean\n",
    "    by_box = d_out.groupby(['BoxID'])['duration_hrs'].mean()\n",
    "    \n",
    "    return by_box.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2 = compute_mean_duration (d, '2016-10-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select events earlier than cut off date\n",
    "date = pd.Timestamp('2016-10-30')\n",
    "\n",
    "d_date = d[d.date_powerfailure <= date]\n",
    "    \n",
    "    #selec only powerout events\n",
    "d_out = d[d.duration_hrs.notnull()]\n",
    "    \n",
    "#Exclude all outages longer than 2 days ()\n",
    "    \n",
    "print ('Leaving out %s ' %(len(d_out[d_out.duration_hrs>=24])))\n",
    "    \n",
    "#Group by box id and compute mean\n",
    "    \n",
    "by_box = d_out.groupby(['BoxID'])['duration_hrs'].mean()\n",
    "\n",
    "return by_box.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict whether there was a power failure in the past 1 day**\n",
    "================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Collapse data to daily level\n",
    "#For all the hours of the day, I simply take the maximum POWERout val\n",
    "d_dy = pd.DataFrame(d.groupby(['BoxID','date_powerfailure'])['POWERout'].max())\n",
    "d_dy.reset_index(inplace=True)\n",
    "d_dy.rename(columns={'POWERout': 'pwrout_1dy'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_dy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train_data (d_raw,date,num_days,target):\n",
    "    \"\"\"\n",
    "    This function returns a training dataset for prediction.\n",
    "\n",
    "    @param d_raw: reasonably preprocessed data\n",
    "    @param date: date threshold (when to stop) e.g., 2016/10/20\n",
    "    @param num_days: Number of days to consider\n",
    "    @return: Returns a dataset where each row represents a time interval(num_days) with corresponding\n",
    "    @raise keyError: raises an exception\n",
    "    \"\"\"\n",
    "    \n",
    "    #create a list of num_days time intervals covering 2016 october-1 up date\n",
    "    start = d_raw.date_powerfailure.min().date()\n",
    "    end = datetime.strptime(date,'%Y/%m/%d').date()\n",
    "    delta = timedelta(days=num_days)\n",
    "    date_list = []\n",
    "\n",
    "    while start <= end:\n",
    "        tup = (start,start+delta)\n",
    "        date_list.append (tup)\n",
    "\n",
    "        start +=  timedelta(days=num_days+ 1)\n",
    "        \n",
    "    #Use cartesian product to create a dataframe of the dates created above\n",
    "    #and boxID's\n",
    "    date_boxid = pd.DataFrame(list(itertools.product(date_list,list(d_raw.BoxID.unique()))),columns=['dates','BoxID'])\n",
    "    \n",
    "    result =[]\n",
    "    d_10dy = pd.DataFrame\n",
    "    for date in list(date_list):\n",
    "        #Select events in this range\n",
    "        mask = (d_raw['date_powerfailure'] >= date[0]) & (d_raw['date_powerfailure'] <= date[1])\n",
    "        d_raw_within = d_raw.loc[mask]\n",
    "        \n",
    "        #Summarise powerouts to num_days level\n",
    "        d_raw_by_box = pd.DataFrame(d_raw_within.groupby(['BoxID'])[target].max())\n",
    "        d_raw_by_box.reset_index(inplace=True)\n",
    "        \n",
    "        new_powerout_col = 'pwrout_' + str(num_days) + 'dy'\n",
    "        d_raw_by_box.rename(columns={target: new_powerout_col},inplace=True)\n",
    "        d_raw_by_box['dates'] = [date for i in range(d_raw_by_box.shape[0])]\n",
    "        \n",
    "        #create a df of summarised power failure events\n",
    "        if len (result) == 0:\n",
    "            d_10dy = d_raw_by_box\n",
    "        else:\n",
    "            d_10dy = d_10dy.append (d_raw_by_box)\n",
    "        \n",
    "        result.append (d_raw_by_box)\n",
    "            \n",
    "    #print (date_boxid.head())\n",
    "    d2 = pd.merge (left = bx, right=d_10dy,on='BoxID', how='right')\n",
    "    \n",
    "    #Rearrange columns\n",
    "    d2 = d2 [['dates','ClusterId','BoxID','LONG','LAT','pwrout_10dy']]\n",
    "    \n",
    "    #rename soem cols\n",
    "    d2.rename (columns={'LONG': 'lon','LAT':'lat'},inplace=True)\n",
    "    return d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "def calculate_distance (pt1,pt2):\n",
    "    \"\"\"\n",
    "    Computes distance between two geographic coordinates\n",
    "    :param pt1: [Lat,Lon] for first point\n",
    "    :param pt2: [Lat,Lon] for second\n",
    "    :returns distance in km between the two points\n",
    "    \"\"\"\n",
    "    # Radius of the earth in km (Hayford-Ellipsoid)\n",
    "    EARTH_RADIUS = 6378388/1000\n",
    "    \n",
    "    d_lat = radians (pt1[0] - pt2[0])\n",
    "    d_lon = radians (pt1[1] - pt2[1])\n",
    "    \n",
    "    lat1 = radians(pt1[0])\n",
    "    lat2 = radians(pt2[0])\n",
    "    \n",
    "    a = sin(d_lat/2) * sin(d_lat/2) + \\\n",
    "        sin(d_lon/2) * sin(d_lon/2) * cos(lat1) * cos(lat2)\n",
    "        \n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "    return c * EARTH_RADIUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_nearest_neighbor (poi,points,target):\n",
    "    \"\"\"\n",
    "    Computes distance between a point of interest and the rest of the points\n",
    "    :param pt1: [Lat,Lon] for first point\n",
    "    :param pt2: [Lat,Lon] for second\n",
    "    :returns a dict object with BoxID and distances\n",
    "    \"\"\"\n",
    "    #dict object to \n",
    "    distances = {}\n",
    "    min = 500000.000\n",
    "    min_bx = ''\n",
    "    predicted_pwrstate = ''\n",
    "    \n",
    "    for bx in list(points.BoxID.unique()):\n",
    "        d_bx = points[points.BoxID == bx]\n",
    "        dist = calculate_distance (poi,[d_bx.lat,d_bx.lon])\n",
    "        \n",
    "        if dist < min:\n",
    "            min = dist\n",
    "            predicted_pwrstate = d_bx[target].values[0]\n",
    "            min_bx = bx\n",
    "    \n",
    "    #print (predicted_pwrstate)\n",
    "    return [min_bx, predicted_pwrstate, min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inorder to get location of boxes, I merge the d_dy dataframe with boxes details\n",
    "d3 = pd.merge (left = d2, right=bx, on='BoxID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_nearest_neighbor2(df, target, num_neighbors, how):\n",
    "    \"\"\"\n",
    "    Given a dataframe with lat/lon and average duration of power failure for that location\n",
    "    this function finds the nearest point to the target point and return the avg power duration\n",
    "    from the nearest point.\n",
    "    :param df: pd.DataFrame with cols: BoxID, lat/lon, avg power failure duration (hours) \n",
    "    :param target: [lat,lon]-The unsampled location where the avg power failure duration needs to be estimated\n",
    "    :param num_neighbors: NUmber of neighbors to use for the nearest neigbor\n",
    "    :param how: The approach to use. If num_neighbors == 1, just return value of nearest point..otherwise, it can\n",
    "    be 'mean', 'mode' or 'median'.\n",
    "    :returns the avg power failure duration and posssibly the BoxID\n",
    "    \"\"\"\n",
    "    #Just incase, remove all duplicate lat/lon pairs\n",
    "    df = df.drop_duplicates (['lon','lat'])\n",
    "    \n",
    "    df ['dist'] = df.apply(lambda row: calculate_distance([row['lat'],row['lon']],target),axis=1)\n",
    "    \n",
    "    #In the simplest case, assign to this new location valaue based on a single nearest neighbor\n",
    "    if num_neighbors==1:\n",
    "        min_dist = df.dist.min()\n",
    "        predicted = df[df.dist==min_dist].duration_hrs.values[0]\n",
    "        return predicted\n",
    "    else: #cases with more than one neighbor\n",
    "        #Select only the 'num_neaighbors' nearest neighbors\n",
    "        df = df.sort_values (by = ['dist'], ascending=True)[:num_neighbors]\n",
    "        if how=='wmean':\n",
    "            #Add weight based on distance\n",
    "            df['dist_wght'] = 1/df['dist']\n",
    "            \n",
    "            #compute simple weighted mean\n",
    "            wmean = (df['duration_hrs'] * df['dist_wght']).sum() /df.dist_wght.sum()\n",
    "            \n",
    "            return wmean\n",
    "        elif how == 'mean':\n",
    "            return df['duration_hrs'].mean (axis=0)\n",
    "        elif how == 'median':\n",
    "            return df['duration_hrs'].median (axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_nearest = predict_with_nearest_neighbor2(d3,[38.694671,69.600202],1,'mean')\n",
    "pred_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_5_mean = predict_with_nearest_neighbor2(d3,[38.694671,69.600202],5,'mean')\n",
    "pred_5_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_5_wmean = predict_with_nearest_neighbor2(d3,[38.694671,69.600202],5,'wmean')\n",
    "pred_5_wmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3[d3.dist==min_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_mean (df, idw):\n",
    "    #add weighted duration-weights is 1/distance\n",
    "    df['dist_wght'] = 1/df['dist']\n",
    "    \n",
    "    #compute mean based on weighted duration\n",
    "    mean = df.['duration_hrs_wgted'].mean()\n",
    "    \n",
    "    if idw:\n",
    "        wmean = (df['duration_hrs'] * df['dist_wght']).sum() /df.dist_wght.sum()\n",
    "        \n",
    "        return wmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['duration_hrs_wgted'] = df.apply (lambda x: 1/x['dist'] * x['duration_hrs'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.duration_hrs_wgted.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['dist_wght'] = 1/df['dist']\n",
    "\n",
    "(df['duration_hrs'] * df['dist_wght']).sum() /df.dist_wght.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.duration_hrs.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally lets see perfomance on separate locations....**\n",
    "============================================================\n",
    "Note that this evaluation is being done on sampled locations...just leaving them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inorder to get location of boxes, I merge the d_dy dataframe with boxes details\n",
    "d2 = pd.merge (left = d_dy, right=bx, on='BoxID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Export this data for quick demo\n",
    "d2.to_csv('../01.data/powerouts_1dy.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A batch Evaluation\n",
    "evaluate_nearest_neighbor_predictor(d2, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folium?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Make predictions on random locaitons on a map**\n",
    "==================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2geojson2(df):\n",
    "    features = []\n",
    "    df.apply(lambda X: features.append( \n",
    "            geojson.Feature(geometry=geojson.Point((X[\"lon\"], \n",
    "                                                    X[\"lat\"])), \n",
    "                properties=dict(ClusterID=X[\"ClusterId\"], \n",
    "                                avg_duration=X[\"duration_hrs\"]))\n",
    "                                    )\n",
    "            , axis=1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = data2geojson2 (by_loc.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f[0].properties['avg_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coords = d2 [['lon','lat', 'ClusterId', 'BoxID']]\n",
    "coords = coords.sort_values(by  = ['lon','lat'] )\n",
    "coords['xy'] = coords.apply(lambda x: (x['lon'], x['lat']),axis=1)\n",
    "xy = coords.xy.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = coords.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('Predicted power state-%s  from box at distance %s km '%(predicted[1],predicted[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boxid = predicted[0]\n",
    "\n",
    "box_loc = [d2[d2.BoxID==boxid].lon,d2[d2.BoxID==boxid].lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy = pd.read_csv('../01.data/powerouts_1dy.csv')\n",
    "\t\n",
    "features = data2geojson2(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_boxes_location():\n",
    "    \"\"\"Get location of the boxes and them to the map\"\"\"\n",
    "    features = []\n",
    "    df = pd.read_csv(data_path + 'powerouts_1dy.csv')\n",
    "    df.apply(lambda X: features.append( \n",
    "            geojson.Feature(geometry=geojson.Point((X[\"lon\"],X[\"lat\"])), \n",
    "            properties=dict(name=X[\"BoxID\"], description=X[\"ClusterId\"])))\n",
    "            , axis=1)\n",
    "    \n",
    "    return geojson.dumps(geojson.FeatureCollection(features), sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
